"""
Step 5-1: LangChain ê¸°ë³¸ - invokeì™€ ë©”ì‹œì§€ ê´€ë¦¬

LangChainì˜ ê°€ìž¥ ê¸°ë³¸ì ì¸ ì‚¬ìš©ë²•ì„ ë‹¨ê³„ë³„ë¡œ í•™ìŠµí•©ë‹ˆë‹¤.
ìžì„¸í•œ ì„¤ëª…ì€ 01_langchain_basic.md íŒŒì¼ì„ ì°¸ê³ í•˜ì„¸ìš”.
"""

import os
from pathlib import Path
from dotenv import load_dotenv
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage

# í™˜ê²½ ì„¤ì •
project_root = Path(__file__).parent.parent
load_dotenv(project_root / ".env")

api_key = os.environ.get("GEMINI_API_KEY")
if not api_key:
    print("âŒ GEMINI_API_KEYë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    exit(1)

print("=" * 70)
print("LangChain ê¸°ë³¸ - invokeì™€ ë©”ì‹œì§€ ê´€ë¦¬")
print("=" * 70)
print()

# ============================================================
# Step 1: ëª¨ë¸ ì´ˆê¸°í™”
# ============================================================
print("ðŸ“Œ Step 1: ëª¨ë¸ ì´ˆê¸°í™”")
print("-" * 70)

llm = ChatGoogleGenerativeAI(
    model="gemini-2.0-flash-exp",
    temperature=0.7,
    max_output_tokens=1000,
    google_api_key=api_key
)

print("âœ… ChatGoogleGenerativeAI ëª¨ë¸ ìƒì„± ì™„ë£Œ")
print(f"   ëª¨ë¸: gemini-2.0-flash-exp")
print(f"   Temperature: 0.7")
print()

# ============================================================
# Step 2: ê¸°ë³¸ invoke (ë¬¸ìžì—´ ì§ì ‘ ì „ë‹¬)
# ============================================================
print("ðŸ“Œ Step 2: ê¸°ë³¸ invoke - ë¬¸ìžì—´ ì§ì ‘ ì „ë‹¬")
print("-" * 70)

question = "íŒŒì´ì¬ì˜ ì£¼ìš” íŠ¹ì§• 3ê°€ì§€ë¥¼ ê°„ë‹¨ížˆ ì•Œë ¤ì¤˜"
print(f"ì§ˆë¬¸: {question}")

response = llm.invoke(question)
print(f"\níƒ€ìž…: {type(response).__name__}")
print(f"ë‚´ìš©: {response.content[:100]}...")
print()

# ============================================================
# Step 3: ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ë¡œ ì „ë‹¬ (êµ¬ì¡°í™”)
# ============================================================
print("ðŸ“Œ Step 3: ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ë¡œ ì „ë‹¬ (ì¶”ì²œ ë°©ì‹)")
print("-" * 70)

messages = [
    SystemMessage(content="ë„ˆëŠ” ì¹œì ˆí•œ í”„ë¡œê·¸ëž˜ë° íŠœí„°ìž…ë‹ˆë‹¤. ì´ˆë³´ìžë„ ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…í•˜ì„¸ìš”."),
    HumanMessage(content="Pythonì˜ ë¦¬ìŠ¤íŠ¸ëž€ ë¬´ì—‡ì¸ê°€ìš”?")
]

response = llm.invoke(messages)
print(f"ì‘ë‹µ:\n{response.content[:200]}...")
print()

# ============================================================
# Step 4: ëŒ€í™” ížˆìŠ¤í† ë¦¬ ê´€ë¦¬ (í•µì‹¬!)
# ============================================================
print("ðŸ“Œ Step 4: ëŒ€í™” ížˆìŠ¤í† ë¦¬ ê´€ë¦¬ (ë©€í‹°í„´ ëŒ€í™”)")
print("-" * 70)

# ìƒˆë¡œìš´ ëŒ€í™” ì‹œìž‘
conversation = [
    SystemMessage(content="ë„ˆëŠ” ì¹œì ˆí•œ í”„ë¡œê·¸ëž˜ë° íŠœí„°ìž…ë‹ˆë‹¤."),
    HumanMessage(content="ë¦¬ìŠ¤íŠ¸ì™€ íŠœí”Œì˜ ì°¨ì´ëŠ” ë­”ê°€ìš”?")
]

print(f"í˜„ìž¬ ë©”ì‹œì§€ ìˆ˜: {len(conversation)}")
response1 = llm.invoke(conversation)
print(f"AI ì‘ë‹µ: {response1.content[:100]}...")

# ì¤‘ìš”! AI ì‘ë‹µì„ ížˆìŠ¤í† ë¦¬ì— ì¶”ê°€í•´ì•¼ ë§¥ë½ ìœ ì§€
conversation.append(response1)
print(f"\nAI ì‘ë‹µ ì¶”ê°€ í›„ ë©”ì‹œì§€ ìˆ˜: {len(conversation)}")

# í›„ì† ì§ˆë¬¸ ì¶”ê°€
conversation.append(HumanMessage(content="ê·¸ëŸ¼ ì–¸ì œ íŠœí”Œì„ ì‚¬ìš©í•˜ë‚˜ìš”?"))
print(f"ì§ˆë¬¸ ì¶”ê°€ í›„ ë©”ì‹œì§€ ìˆ˜: {len(conversation)}")

# "ê·¸ëŸ¼"ì´ë¼ëŠ” ë‹¨ì–´ ì‚¬ìš© ê°€ëŠ¥ (ì´ì „ ë§¥ë½ ê¸°ì–µ)
response2 = llm.invoke(conversation)
print(f"AI ì‘ë‹µ: {response2.content[:150]}...")
print()

# ============================================================
# Step 5: ëŒ€í™” ížˆìŠ¤í† ë¦¬ ì‹œê°í™”
# ============================================================
print("ðŸ“Œ Step 5: í˜„ìž¬ ëŒ€í™” ížˆìŠ¤í† ë¦¬ í™•ì¸")
print("-" * 70)

conversation.append(response2)  # ë§ˆì§€ë§‰ ì‘ë‹µë„ ì¶”ê°€

print(f"ì´ ë©”ì‹œì§€ ìˆ˜: {len(conversation)}")
print("\nëŒ€í™” íë¦„:")
for i, msg in enumerate(conversation):
    role = msg.__class__.__name__
    icon = "ðŸ¤–" if role == "AIMessage" else ("ðŸ‘¤" if role == "HumanMessage" else "âš™ï¸")
    content_preview = msg.content[:60] + "..." if len(msg.content) > 60 else msg.content
    print(f"  {icon} [{i+1}] {role:15} {content_preview}")
print()
