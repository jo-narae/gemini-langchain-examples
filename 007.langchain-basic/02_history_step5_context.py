"""
Step 5-2-5: ëŒ€í™” ë§¥ë½ ì—°ê²°

ìƒˆë¡œìš´ ì£¼ì œë¡œ ëŒ€í™”ë¥¼ ì´ì–´ê°€ë©´ì„œ ì´ì „ ë§¥ë½ì„ ìœ ì§€í•˜ëŠ” ë°©ë²•ì„ í•™ìŠµí•©ë‹ˆë‹¤.
"""

import os
import logging
from pathlib import Path
from dotenv import load_dotenv
from langchain_core.chat_history import InMemoryChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_core.messages import HumanMessage
from langchain_google_genai import ChatGoogleGenerativeAI

# Rate Limit ì¬ì‹œë„ ê²½ê³  ë©”ì‹œì§€ ìˆ¨ê¸°ê¸° (ì‹¤ì œ ì—ëŸ¬ëŠ” ì—¬ì „íˆ í‘œì‹œë¨)
logging.getLogger("langchain_google_genai").setLevel(logging.ERROR)

# í™˜ê²½ ì„¤ì •
project_root = Path(__file__).parent.parent
load_dotenv(project_root / ".env")

api_key = os.getenv("GEMINI_API_KEY")
if not api_key:
    print("âŒ GEMINI_API_KEYë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    exit(1)

# ëª¨ë¸ ë° íˆìŠ¤í† ë¦¬ ì‹œìŠ¤í…œ ì„¤ì •
model = ChatGoogleGenerativeAI(
    model="gemini-2.0-flash-exp",
    temperature=0.7,
    google_api_key=api_key,
    max_retries=5,  # ì¬ì‹œë„ íšŸìˆ˜ ì¦ê°€
    request_timeout=60,  # íƒ€ì„ì•„ì›ƒ ì¦ê°€
    model_kwargs={
        "system_instruction": (
            "ë„ˆëŠ” ì‚¬ìš©ìë¥¼ ë„ì™€ì£¼ëŠ” ìƒë‹´ì‚¬ì•¼. ê³µê°ì ìœ¼ë¡œ ë‹µí•˜ê³ , "
            "ëª¨í˜¸í•˜ë©´ ì§§ê²Œ ë˜ë¬¼ì–´ë´. í•„ìš”í•˜ë©´ ë‹¨ê³„ë³„ë¡œ ì•ˆë‚´í•´ì¤˜."
        )
    },
)

store = {}

def get_session_history(session_id: str):
    if session_id not in store:
        store[session_id] = InMemoryChatMessageHistory()
    return store[session_id]

with_message_history = RunnableWithMessageHistory(model, get_session_history)

print("=" * 70)
print("Step 5: ëŒ€í™” ë§¥ë½ ì—°ê²°")
print("=" * 70)
print()

# ============================================================
# ì„¸ì…˜ abc2 ì¬í˜„ (ì „ì²´ ì´ì „ ëŒ€í™”)
# ============================================================
print("ğŸ“Œ 5-1. ì„¸ì…˜ abc2 ì¬í˜„")
print("-" * 70)

config = {"configurable": {"session_id": "abc2"}}

# ì´ì „ ëŒ€í™” ì¬í˜„
with_message_history.invoke([HumanMessage(content="ì•ˆë…•? ë‚œ ê¹€ì² ìˆ˜ì´ì•¼.")], config)
with_message_history.invoke([HumanMessage(content="ë‚´ ì´ë¦„ì´ ë­ì§€?")], config)
with_message_history.invoke([HumanMessage(content="ë‚´ê°€ ì–´ëŠ ë‚˜ë¼ ì‚¬ëŒì¸ì§€ ì¶”ì¸¡í•˜ê³ , ê·¸ ë‚˜ë¼ ë¬¸í™” í•œ ê°€ì§€ë¥¼ ë§í•´ì¤˜")], config)

print("âœ… ì„¸ì…˜ abc2 ì¤€ë¹„ ì™„ë£Œ")
print(f"   í˜„ì¬ ë©”ì‹œì§€ ìˆ˜: {len(store['abc2'].messages)}ê°œ")
print()

# ============================================================
# ìƒˆë¡œìš´ ì£¼ì œë¡œ ëŒ€í™”
# ============================================================
print("ğŸ“Œ 5-2. ìƒˆë¡œìš´ ì£¼ì œë¡œ ëŒ€í™”")
print("-" * 70)

print("ğŸ‘¤ ì‚¬ìš©ì: ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì¢‹ë‹¤ë©´ ë­˜ í•˜ë©´ ì¢‹ì„ê¹Œ?")
print("ğŸ¤– AI ì‘ë‹µ ì¤‘...")
print()

response = with_message_history.invoke(
    [HumanMessage(content="ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì¢‹ë‹¤ë©´ ë­˜ í•˜ë©´ ì¢‹ì„ê¹Œ?")],
    config,
)

print(f"ğŸ¤– AI: {response.content}")
print()

# ============================================================
# ì´ì „ ëŒ€í™” ë§¥ë½ ì—°ê²°
# ============================================================
print("ğŸ“Œ 5-3. ì´ì „ ëŒ€í™” ë§¥ë½ ì—°ê²°")
print("-" * 70)

print("ğŸ‘¤ ì‚¬ìš©ì: ì•„ê¹Œ ë‚´ ì´ë¦„ê³¼ í•¨ê»˜ ì¶”ì²œí•´ì¤„ ìˆ˜ ìˆì–´?")
print("ğŸ¤– AI ì‘ë‹µ ì¤‘...")
print()

response = with_message_history.invoke(
    [HumanMessage(content="ì•„ê¹Œ ë‚´ ì´ë¦„ê³¼ í•¨ê»˜ ì¶”ì²œí•´ì¤„ ìˆ˜ ìˆì–´?")],
    config,
)

print(f"ğŸ¤– AI: {response.content}")
print()

# ============================================================
# ì „ì²´ ëŒ€í™” ê¸°ë¡ í™•ì¸
# ============================================================
print("ğŸ“Œ 5-4. ì „ì²´ ëŒ€í™” ê¸°ë¡")
print("-" * 70)

print(f"ğŸ’¬ ì„¸ì…˜ 'abc2'ì˜ ì „ì²´ ëŒ€í™”:")
print()
for i, message in enumerate(store['abc2'].messages, 1):
    speaker = "ğŸ‘¤ ì‚¬ìš©ì" if message.__class__.__name__ == "HumanMessage" else "ğŸ¤– AI"
    content = message.content[:80] + "..." if len(message.content) > 80 else message.content
    print(f"{i}. {speaker}: {content}")
print()
