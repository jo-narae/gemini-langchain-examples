"""
Step 5-0: ì™œ LangChainì„ ì‚¬ìš©í•˜ë‚˜?

Gemini APIë¥¼ ì§ì ‘ ì‚¬ìš©í•˜ëŠ” ê²ƒê³¼ LangChainì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì˜ ì°¨ì´ë¥¼ ë¹„êµí•©ë‹ˆë‹¤.
ìì„¸í•œ ì„¤ëª…ì€ 00_why_langchain.md íŒŒì¼ì„ ì°¸ê³ í•˜ì„¸ìš”.
"""

import os
from pathlib import Path
from dotenv import load_dotenv
from google import genai
from google.genai import types
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import HumanMessage, SystemMessage

# í™˜ê²½ ì„¤ì •
project_root = Path(__file__).parent.parent
load_dotenv(project_root / ".env")

api_key = os.getenv("GEMINI_API_KEY")
if not api_key:
    print("âŒ GEMINI_API_KEYë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    exit(1)

print("=" * 70)
print("ì™œ LangChainì„ ì‚¬ìš©í•˜ë‚˜? - Gemini API vs LangChain ë¹„êµ")
print("=" * 70)
print()

# ============================================================
# Part 1: Gemini API ì§ì ‘ ì‚¬ìš© (ìµœì‹  google-genai SDK)
# ============================================================
print("ğŸ“Œ Part 1: Gemini API ì§ì ‘ ì‚¬ìš©")
print("=" * 70)

client = genai.Client(api_key=api_key)
system_instruction = "ë„ˆëŠ” ì¹œì ˆí•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."

# ë‹¨ì¼ ì§ˆë¬¸
print("\nâœ… ë‹¨ì¼ ì§ˆë¬¸:")
response = client.models.generate_content(
    model=os.getenv("GEMINI_MODEL", "gemini-2.5-flash-lite"),
    contents="íŒŒì´ì¬ì´ë€?",
    config=types.GenerateContentConfig(system_instruction=system_instruction)
)
print(f"ì‘ë‹µ: {response.text[:100]}...")

# ëŒ€í™” íˆìŠ¤í† ë¦¬ ê´€ë¦¬
print("\nâœ… ëŒ€í™” íˆìŠ¤í† ë¦¬ ê´€ë¦¬:")
history = []

# ì²« ë²ˆì§¸ ì§ˆë¬¸
history.append(types.Content(role="user", parts=[types.Part(text="íŒŒì´ì¬ì´ë€?")]))
response1 = client.models.generate_content(
    model=os.getenv("GEMINI_MODEL", "gemini-2.5-flash-lite"),
    contents=history,
    config=types.GenerateContentConfig(system_instruction=system_instruction)
)
print(f"Q1: íŒŒì´ì¬ì´ë€?")
print(f"A1: {response1.text[:80]}...")
history.append(types.Content(role="model", parts=[types.Part(text=response1.text)]))

# ë‘ ë²ˆì§¸ ì§ˆë¬¸
history.append(types.Content(role="user", parts=[types.Part(text="ê·¸ëŸ¼ ìë°”ëŠ”?")]))
response2 = client.models.generate_content(
    model=os.getenv("GEMINI_MODEL", "gemini-2.5-flash-lite"),
    contents=history,
    config=types.GenerateContentConfig(system_instruction=system_instruction)
)
print(f"Q2: ê·¸ëŸ¼ ìë°”ëŠ”?")
print(f"A2: {response2.text[:80]}...")

# ============================================================
# Part 2: LangChain ì‚¬ìš© (ê¶Œì¥ ë°©ì‹)
# ============================================================
print("\n\nğŸ“Œ Part 2: LangChain ì‚¬ìš©")
print("=" * 70)

langchain_model = ChatGoogleGenerativeAI(
    model=os.getenv("GEMINI_MODEL", "gemini-2.5-flash-lite"),
    temperature=0.7,
    google_api_key=api_key
)

# ë‹¨ì¼ ì§ˆë¬¸
print("\nâœ… ë‹¨ì¼ ì§ˆë¬¸:")
response = langchain_model.invoke("íŒŒì´ì¬ì´ë€?")
print(f"ì‘ë‹µ: {response.content[:100]}...")

# ëŒ€í™” íˆìŠ¤í† ë¦¬ ê´€ë¦¬
print("\nâœ… ëŒ€í™” íˆìŠ¤í† ë¦¬ ê´€ë¦¬:")
messages = [
    SystemMessage(content="ë„ˆëŠ” ì¹œì ˆí•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."),
    HumanMessage(content="íŒŒì´ì¬ì´ë€?")
]
response1 = langchain_model.invoke(messages)
print(f"Q1: íŒŒì´ì¬ì´ë€?")
print(f"A1: {response1.content[:80]}...")

messages.append(response1)
messages.append(HumanMessage(content="ê·¸ëŸ¼ ìë°”ëŠ”?"))
response2 = langchain_model.invoke(messages)
print(f"Q2: ê·¸ëŸ¼ ìë°”ëŠ”?")
print(f"A2: {response2.content[:80]}...")
