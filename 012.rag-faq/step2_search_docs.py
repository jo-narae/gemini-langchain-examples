"""
RAG FAQ ì±—ë´‡ - Step 2: ë²¡í„°DB ê²€ìƒ‰ ë° ê´€ë ¨ ë¬¸ì„œ í‘œì‹œ

í•™ìŠµ ëª©í‘œ:
- step1ì˜ ê¸°ëŠ¥ (PDF ì—…ë¡œë“œ ë° ë²¡í„°DB ì €ì¥) í¬í•¨
- ë²¡í„°DBì—ì„œ ìœ ì‚¬í•œ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ëŠ” ë°©ë²•
- ê²€ìƒ‰ëœ ê´€ë ¨ ë¬¸ì„œë¥¼ í™”ë©´ì— í‘œì‹œí•˜ëŠ” ë°©ë²•
"""

import streamlit as st

from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_core.documents.base import Document
from langchain_community.vectorstores import FAISS
from typing import List
import os
from pathlib import Path

# í™˜ê²½ë³€ìˆ˜ ì„¤ì •
from dotenv import load_dotenv
env_path = Path(__file__).parent.parent / ".env"
load_dotenv(dotenv_path=env_path)


############################### ë²¡í„°DBì—ì„œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ ##########################

@st.cache_data
def search_related_docs(user_question: str) -> List[Document]:
    """ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ìœ ì‚¬í•œ ë¬¸ì„œë¥¼ ë²¡í„°DBì—ì„œ ê²€ìƒ‰"""
    # ì„ë² ë”© ëª¨ë¸ ìƒì„± (ì €ì¥í•  ë•Œì™€ ë™ì¼í•œ ëª¨ë¸ ì‚¬ìš©)
    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
    )

    # ë²¡í„° DB ë¡œë“œ
    vector_db = FAISS.load_local("faiss_index", embeddings, allow_dangerous_deserialization=True)

    # ê´€ë ¨ ë¬¸ì„œ 3ê°œ ê²€ìƒ‰
    retriever = vector_db.as_retriever(search_kwargs={"k": 3})
    related_docs: List[Document] = retriever.invoke(user_question)

    return related_docs


############################### Streamlit UI ##########################

def main():
    st.set_page_config("Step 2: ë¬¸ì„œ ê²€ìƒ‰", layout="wide")
    st.header("ğŸ” Step 2: ë²¡í„°DB ê²€ìƒ‰ ë° ê´€ë ¨ ë¬¸ì„œ í‘œì‹œ")

    st.markdown("""
    ### ì´ ë‹¨ê³„ì—ì„œ ë°°ìš°ëŠ” ê²ƒ:
    1. **ì „ì œì¡°ê±´**: step1ì—ì„œ ì´ë¯¸ ë²¡í„°DBê°€ ìƒì„±ë˜ì–´ ìˆì–´ì•¼ í•¨
    2. **ìƒˆë¡œìš´ ë‚´ìš©**: ì‚¬ìš©ì ì§ˆë¬¸ìœ¼ë¡œ ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰
    3. ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ í™”ë©´ì— í‘œì‹œ

    ğŸ’¡ **ì°¸ê³ **: ë¨¼ì € step1ì„ ì‹¤í–‰í•˜ì—¬ ë²¡í„°DBë¥¼ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤!
    """)

    st.divider()

    # ì§ˆë¬¸ ì…ë ¥
    user_question = st.text_input(
        "ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”",
        placeholder="ì˜ˆ) ì²­ì•½ 1ìˆœìœ„ ì¡°ê±´ì´ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?"
    )

    if user_question:
        st.subheader("ğŸ“š ê²€ìƒ‰ëœ ê´€ë ¨ ë¬¸ì„œ")

        # ë²¡í„°DBì—ì„œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
        try:
            related_docs = search_related_docs(user_question)

            # ê´€ë ¨ ë¬¸ì„œ í‘œì‹œ
            for idx, document in enumerate(related_docs, 1):
                with st.expander(f"ğŸ“„ ê´€ë ¨ ë¬¸ì„œ {idx}"):
                    st.write(document.page_content)

                    # ë©”íƒ€ë°ì´í„° í‘œì‹œ
                    file_path = document.metadata.get('file_path', '')
                    page_number = document.metadata.get('page', 0) + 1
                    st.caption(f"ì¶œì²˜: {os.path.basename(file_path)} | í˜ì´ì§€: {page_number}")

            st.info("""
            ### ë‹¤ìŒ ë‹¨ê³„ (step3)ì—ì„œëŠ”:
            - ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ í™œìš©í•˜ì—¬ Geminië¡œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤
            """)

        except Exception as e:
            st.error(f"âŒ ë²¡í„°DBë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € PDFë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.\nì˜¤ë¥˜: {e}")

if __name__ == "__main__":
    main()
